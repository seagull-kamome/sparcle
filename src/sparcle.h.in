// \file sparcle.h
// \brief C header file of sparcle.
//
// Copyright (C) 2016, Hattori, Hiroki all rights reserved.
// This program released under BSD3 license.
//
#if !defined(SPARCLE_H_INCLUDED)
#define SPARCLE_H_INCLUDED

#include <stdbool.h>
#include <stdint.h>
#include <stdatomic.h>
#include <errno.h>


#define SPARCLE_VERSION_MAJOR @SPARCLE_VERSION_MAJOR@
#define SPARCLE_VERSION_MINOR @SPARCLE_VERSION_MINOR@


/* ******************************************************* */
/** \addgroup Utility
 * @{ */

/** \addgroup Spin lock
 * @{ */
typedef atomic_flag volatile sparcle_spin_lock_t;
#define SPARCLE_SPIN_LOCK_UNLOCKED ATOMIC_FLAG_INIT

inline void sparcle_spin_lock_init(sparcle_spin_lock_t* lck) {
  atomic_flag_clear(lck);
}

inline void sparcle_spin_lock(sparcle_spin_lock_t* lck) {
  while (atomic_flag_test_and_set_explicit(lck, memory_order_acquire)) ; // spin
}
inline void sparcle_spin_unlock(sparcle_spin_lock_t* lck) {
  atomic_flag_clear_explicit(lck, memory_order_release);
}
/* @} */

/** \addgroup Read-Write lock
 * @{ */
typedef atomic_uintptr_t volatile sparcle_rw_lock_t;
#define SPARCLE_RW_LOCK_INIT ATOMIC_VAR_INIT(0);

inline void sparcle_rw_lock_init(sparcle_rw_lock_t* lck) {
  atomic_init(lck, 0);
}

inline void sparcle_read_lock(sparcle_rw_lock_t* lck) {
  uintptr_t x = atomic_load_explicit(lck, memory_order_acquire);
  while ((x & 1) != 0 || ! atomic_compare_exchange_weak(lck, &x, x + 2))
    ; // spin
}

inline bool sparcle_try_read_lock(sparcle_rw_lock_t* lck) {
  uintptr_t x = atomic_load_explicit(lck, memory_order_acquire);
  return ((x & 1) == 0) && atomic_compare_exchange_strong(lck, &x, x + 2);
}

inline void sparcle_read_unlock(sparcle_rw_lock_t* lck) { atomic_fetch_sub(lck, 2); }



inline void sparcle_write_lock(sparcle_rw_lock_t* lck) {
  uintptr_t x = atomic_load_explicit(lck, memory_order_acquire);
  while (x != 0 || ! atomic_compare_exchange_weak(lck, &x, 1) )
    ; // spin
}

inline bool sparcle_try_write_lock(sparcle_rw_lock_t* lck) {
  uintptr_t x = atomic_load_explicit(lck, memory_order_acquire);
  return x == 0 && atomic_compare_exchange_strong(lck, &x, 1);
}

inline void sparcle_write_unlock(sparcle_rw_lock_t* lck) {
  atomic_store_explicit(lck, 0, memory_order_release);
}

/** @} */
/** @} */
/* ******************************************************* */
/** \addgroup Scheduler
 * @{ */

typedef struct tag_sparcle_scheduler* sparcle_scheduler_t;
extern _Thread_local sparcle_scheduler_t sparcle_current_scheduler;

sparcle_scheduler_t sparcle_scheduler_new(size_t num_workers);
void sparcle_scheduler_delete(sparcle_scheduler_t sched);
void sparcle_scheduler_stop(sparcle_scheduler_t sched);
inline bool sparcle_is_worker_thread() { return sparcle_current_scheduler != NULL; }

/** @} */
/* ******************************************************* */
/** \addgroup Thread
 * @{ */

typedef struct tag_sparcle_thread* sparcle_thread_t;
typedef int sparcle_attr_t; // FIXME
typedef enum {
  SPARCLE_THREAD_STATE_RUN, SPARCLE_THREAD_STATE_IDLE,
  SPARCLE_THREAD_STATE_WAIT,
  SPARCLE_THREAD_STATE_SUSPEND, SPARCLE_THREAD_STATE_EXIT
} sparcle_thread_state_t;

extern _Thread_local sparcle_thread_t sparcle_current_thread;

inline int sparcle_attr_init(sparcle_attr_t* attr) { return 0; } // FIXME:
inline int sparcle_attr_destroy(sparcle_attr_t* attr) { return 0; } // FIXME:

int sparcle_thread_new(
        sparcle_scheduler_t sched,
        sparcle_thread_t* thread, sparcle_attr_t const* attr,
        void* (*start_routine)(void*), void* arg);
void sparcle_thread_exit(void* retval);
int sparcle_thread_suspend();
void sparcle_thread_wakeup(sparcle_thread_t thread);


/** @} */
/* ******************************************************* */
/** \addgroup Wait queue
 * @{ */

typedef struct {
  atomic_uintptr_t q_;
} sparcle_wait_queue_t;

#define SPARCLE_WAIT_QUEUE_INITIALIZER { ATOMIC_INIT((uintptr_t)NULL) }
inline void sparcle_wait_queue_init(sparcle_wait_queue_t* q) {
  atomic_init(&q->q_, (uintptr_t)NULL);
}

/** キューで待つ
 * @param q 待ち合わせを行うキュー
 */
void sparcle_wait_queue_wait(sparcle_wait_queue_t* q);

/** キューで待っているスレッドを１つ起床させる
 * @param q 対象のキュー
 * @return 起床できたらそのスレッドを返す
 */
sparcle_thread_t sparcle_wait_queue_signal(sparcle_wait_queue_t* q);

/** キューで待っているスレッドを全て起床させる
 * @param q 対象のキュー
 */
void sparcle_wait_queue_broadcast(sparcle_wait_queue_t* q);

/** @} */
/* ******************************************************* */
/** \addgroup Mutex
 * @{ */
typedef struct {
  sparcle_wait_queue_t wait_queue_;
  atomic_flag flag_;
} sparcle_mutex_t;


#define SPARCLE_MUTEX_INITIALIZER { SPARCLE_WAIT_QUEUE_INITIALIZER, ATOMIC_FLAG_INIT }
// #define SPARCLE_RECURSIVE_MUTEX_INITIALIZER_NP
// #define SPARCLE_ERRORCHECK_MUTEXT_INITIALIZER_NP
typedef int sparcle_mutexattr_t;
inline int sparcle_mutexattr_init(sparcle_mutexattr_t* attr) { return 0; }
inline int sparcle_mutexattr_destroy(sparcle_mutexattr_t* attr) { return 0; }

inline int sparcle_mutex_init(sparcle_mutex_t *mutex, const pthread_mutexattr_t *mutexattr) {
  sparcle_wait_queue_init(&mutex->wait_queue_);
  atomic_flag_clear(&mutex->flag_);
  return 0;
}

int sparcle_mutex_lock(sparcle_mutex_t *mutex);
inline int sparcle_mutex_trylock(sparcle_mutex_t *mutex) {
  return atomic_flag_test_and_set(&mutex->flag_) == false;
}
inline int sparcle_mutex_unlock(sparcle_mutex_t *mutex);
inline int sparcle_mutex_destroy(sparcle_mutex_t *mutex) { return 0; }


/** @} */
/* ******************************************************* */
/** \addgroup Condition variable
 * @{ */
typedef struct {
  sparcle_wait_queue_t wait_queue_;
} sparcle_cond_t;

#define SPARCLE_COND_INITIALIZER { SPARCLE_WAIT_QUEUE_INITIALIZER }

typedef int sparcle_condattr_t;
inline int sparcle_condattr_init(sparcle_condattr_t* attr) { return 0; }
inline int sparcle_condattr_destroy(sparcle_condattr_t* attr) { return 0; }

inline int sparcle_cond_init(sparcle_cond_t* cond, sparcle_condattr_t* cond_attr) {
  sparcle_wait_queue_init(&cond->wait_queue_);
  return 0;
}

inline int sparcle_cond_signal(sparcle_cond_t *cond) {
  sparcle_wait_queue_signal(&cond->wait_queue_);
  return 0;
}
inline int sparcle_cond_broadcast(sparcle_cond_t *cond) {
  sparcle_wait_queue_broadcast(&cond->wait_queue_);
}

int sparcle_cond_wait(sparcle_cond_t *cond, sparcle_mutex_t *mutex);

// int sparcle_cond_timedwait(sparcle_cond_t *cond, sparcle_mutex_t *mutex, const struct timespec *abstime);

inline int sparcle_cond_destroy(sparcle_cond_t *cond) {
  return (atomic_load(&cond->wait_queue_.q_) != (uintptr_t)NULL)? EBUSY : 0;
}


/** @} */
/* ******************************************************* */
/** \addgroup Semaphore
 * @{ */
typedef struct {
  sparcle_mutex_t mutex_;
  sparcle_cond_t cond_;
  atomic_uint current_;
} sparcle_semaphore_t;


#define SPARCLE_SEMAPHORE_INITIALIZER(n) { SPARCLE_MUTEX_INITIALIZER, SPARCLE_COND_INITIALIZER, ATOMIC_VAR_INIT(n) }
typedef int sparcle_semattr_t;
inline int sparcle_semattr_init(sparcle_semattr_t* attr) { return 0; }
inline int sparcle_semattr_destroy(sparcle_semattr_t* attr) { return 0; }

inline int sparcle_semaphore_init(sparcle_semaphore_t* semaphore, size_t n, sparcle_semattr_t* attr)
{
  int r = sparcle_mutex_init(&semaphore->mutex_, NULL);
  if (r) return r;
  r = sparcle_cond_init(&semaphore->cond_, NULL);
  if (r) {
    sparcle_mutex_destroy(&semaphore->mutex_);
    return r;
  }
  atomic_store(&semaphore->current_, n);
  return 0;
}

int sparcle_semaphore_get(sparcle_semaphore_t* semaphore, size_t n);
inline int sparcle_semaphore_tryget(sparcle_semaphore_t* semaphore, size_t n) {
  unsigned int x = atomic_load(&semaphore->current_);
  while (x >= n) {
    if (atomic_compare_exchange_weak(&semaphore->current_, &x, x - n))
      return 0;
  }
  return EBUSY;
}


inline int sparcle_semaphore_release(sparcle_semaphore_t* semaphore, size_t n) {
  atomic_fetch_add(&semaphore->current_, n);
  return 0;
}

inline int sparcle_semaphore_destroy(sparcle_semaphore_t* semaphore) {
  int r = sparcle_cond_destroy(&semaphore->cond_);
  if (r != 0) return r;
  sparcle_mutex_destroy(&semaphore->mutex_);
  return 0;
}


/** @} */
#endif
/* vim: ts=8 sw=2 tw=0 cindent
*/

